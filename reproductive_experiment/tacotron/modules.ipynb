{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from hyperparams import Hyperparams as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "       [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  tf.concat in embed\n",
    "\n",
    "n = 10\n",
    "lookup_table = tf.placeholder(tf.int32, name = 'lookup_table')\n",
    "pad = tf.zeros(shape=[1, n], dtype=tf.int32)\n",
    "g = tf.concat([pad, lookup_table[1:, :]], axis=0)\n",
    "\n",
    "lt = [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "         [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "         [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    a = sess.run(g, feed_dict={lookup_table: lt})\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function embedding_lookup in module tensorflow.python.ops.embedding_ops:\n",
      "\n",
      "embedding_lookup(params, ids, partition_strategy='mod', name=None, validate_indices=True, max_norm=None)\n",
      "    Looks up `ids` in a list of embedding tensors.\n",
      "    \n",
      "    This function is used to perform parallel lookups on the list of\n",
      "    tensors in `params`.  It is a generalization of\n",
      "    [`tf.gather()`](../../api_docs/python/array_ops.md#gather), where `params` is\n",
      "    interpreted as a partitioning of a large embedding tensor.  `params` may be\n",
      "    a `PartitionedVariable` as returned by using `tf.get_variable()` with a\n",
      "    partitioner.\n",
      "    \n",
      "    If `len(params) > 1`, each element `id` of `ids` is partitioned between\n",
      "    the elements of `params` according to the `partition_strategy`.\n",
      "    In all strategies, if the id space does not evenly divide the number of\n",
      "    partitions, each of the first `(max_id + 1) % len(params)` partitions will\n",
      "    be assigned one more id.\n",
      "    \n",
      "    If `partition_strategy` is `\"mod\"`, we assign each id to partition\n",
      "    `p = id % len(params)`. For instance,\n",
      "    13 ids are split across 5 partitions as:\n",
      "    `[[0, 5, 10], [1, 6, 11], [2, 7, 12], [3, 8], [4, 9]]`\n",
      "    \n",
      "    If `partition_strategy` is `\"div\"`, we assign ids to partitions in a\n",
      "    contiguous manner. In this case, 13 ids are split across 5 partitions as:\n",
      "    `[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12]]`\n",
      "    \n",
      "    The results of the lookup are concatenated into a dense\n",
      "    tensor. The returned tensor has shape `shape(ids) + shape(params)[1:]`.\n",
      "    \n",
      "    Args:\n",
      "      params: A single tensor representing the complete embedding tensor,\n",
      "        or a list of P tensors all of same shape except for the first dimension,\n",
      "        representing sharded embedding tensors.  Alternatively, a\n",
      "        `PartitionedVariable`, created by partitioning along dimension 0. Each\n",
      "        element must be appropriately sized for the given `partition_strategy`.\n",
      "      ids: A `Tensor` with type `int32` or `int64` containing the ids to be looked\n",
      "        up in `params`.\n",
      "      partition_strategy: A string specifying the partitioning strategy, relevant\n",
      "        if `len(params) > 1`. Currently `\"div\"` and `\"mod\"` are supported. Default\n",
      "        is `\"mod\"`.\n",
      "      name: A name for the operation (optional).\n",
      "      validate_indices: Whether or not to validate gather indices.\n",
      "      max_norm: If not None, embedding values are l2-normalized to the value of\n",
      "       max_norm.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` with the same type as the tensors in `params`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If `params` is empty.\n",
      "\n",
      "lookup_table = [[-0.45359232 -0.85074823  0.34374631]\n",
      " [-0.91181458  0.22338264 -1.71117524]\n",
      " [ 0.95973502 -0.59637926  0.63283693]\n",
      " [-0.93767676 -1.48733031 -0.73300532]\n",
      " [ 0.16576793 -0.09820374 -0.75246729]\n",
      " [ 0.78629857 -1.66523214  0.18375831]\n",
      " [ 1.49897316  0.69960007 -3.24587896]\n",
      " [-0.50191115 -0.79932961 -2.06624139]\n",
      " [ 0.72461598  0.96221569 -1.20209793]\n",
      " [ 0.19742772 -0.48094018 -0.10080108]]\n",
      "looked up embeddings = [[ 0.19742772 -0.48094018 -0.10080108]\n",
      " [ 0.72461598  0.96221569 -1.20209793]\n",
      " [-0.50191115 -0.79932961 -2.06624139]\n",
      " [ 1.49897316  0.69960007 -3.24587896]\n",
      " [ 0.78629857 -1.66523214  0.18375831]\n",
      " [ 0.16576793 -0.09820374 -0.75246729]\n",
      " [-0.93767676 -1.48733031 -0.73300532]\n",
      " [ 0.95973502 -0.59637926  0.63283693]\n",
      " [-0.91181458  0.22338264 -1.71117524]\n",
      " [-0.45359232 -0.85074823  0.34374631]]\n"
     ]
    }
   ],
   "source": [
    "# tf.nn.embedding_lookup in embed\n",
    "\n",
    "help(tf.nn.embedding_lookup)\n",
    "\n",
    "vocabulary_size = 10\n",
    "embedding_dimension = 3\n",
    "\n",
    "lookup_table = tf.constant(np.random.normal(size=[vocabulary_size, embedding_dimension]))\n",
    "inputs = tf.placeholder(tf.int32, name='inputs')\n",
    "\n",
    "g = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "\n",
    "input_value = [9,8,7,6,5,4,3,2,1,0]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        lt = sess.run(lookup_table)\n",
    "        a = sess.run(g, feed_dict={inputs: input_value})\n",
    "print('lookup_table = {}'.format(lt))\n",
    "print('looked up embeddings = {}'.format(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function batch_norm in module tensorflow.contrib.layers.python.layers.layers:\n",
      "\n",
      "batch_norm(inputs, decay=0.999, center=True, scale=False, epsilon=0.001, activation_fn=None, param_initializers=None, updates_collections='update_ops', is_training=True, reuse=None, variables_collections=None, outputs_collections=None, trainable=True, batch_weights=None, fused=False, data_format='NHWC', zero_debias_moving_mean=False, scope=None)\n",
      "    Adds a Batch Normalization layer from http://arxiv.org/abs/1502.03167.\n",
      "    \n",
      "      \"Batch Normalization: Accelerating Deep Network Training by Reducing\n",
      "      Internal Covariate Shift\"\n",
      "    \n",
      "      Sergey Ioffe, Christian Szegedy\n",
      "    \n",
      "    Can be used as a normalizer function for conv2d and fully_connected.\n",
      "    \n",
      "    Note: When is_training is True the moving_mean and moving_variance need to be\n",
      "    updated, by default the update_ops are placed in `tf.GraphKeys.UPDATE_OPS` so\n",
      "    they need to be added as a dependency to the `train_op`, example:\n",
      "    \n",
      "      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
      "      if update_ops:\n",
      "        updates = tf.group(*update_ops)\n",
      "        total_loss = control_flow_ops.with_dependencies([updates], total_loss)\n",
      "    \n",
      "    One can set updates_collections=None to force the updates in place, but that\n",
      "    can have speed penalty, specially in distributed settings.\n",
      "    \n",
      "    Args:\n",
      "      inputs: a tensor with 2 or more dimensions, where the first dimension has\n",
      "        `batch_size`. The normalization is over all but the last dimension if\n",
      "        `data_format` is `NHWC` and the second dimension if `data_format` is\n",
      "        `NCHW`.\n",
      "      decay: decay for the moving average. Reasonable values for `decay` are close\n",
      "        to 1.0, typically in the multiple-nines range: 0.999, 0.99, 0.9, etc.\n",
      "        Lower `decay` value (recommend trying `decay`=0.9) if model experiences\n",
      "        reasonably good training performance but poor validation and/or test\n",
      "        performance. Try zero_debias_moving_mean=True for improved stability.\n",
      "      center: If True, add offset of `beta` to normalized tensor. If False, `beta`\n",
      "        is ignored.\n",
      "      scale: If True, multiply by `gamma`. If False, `gamma` is\n",
      "        not used. When the next layer is linear (also e.g. `nn.relu`), this can be\n",
      "        disabled since the scaling can be done by the next layer.\n",
      "      epsilon: small float added to variance to avoid dividing by zero.\n",
      "      activation_fn: activation function, default set to None to skip it and\n",
      "        maintain a linear activation.\n",
      "      param_initializers: optional initializers for beta, gamma, moving mean and\n",
      "        moving variance.\n",
      "      updates_collections: collections to collect the update ops for computation.\n",
      "        The updates_ops need to be executed with the train_op.\n",
      "        If None, a control dependency would be added to make sure the updates are\n",
      "        computed in place.\n",
      "      is_training: whether or not the layer is in training mode. In training mode\n",
      "        it would accumulate the statistics of the moments into `moving_mean` and\n",
      "        `moving_variance` using an exponential moving average with the given\n",
      "        `decay`. When it is not in training mode then it would use the values of\n",
      "        the `moving_mean` and the `moving_variance`.\n",
      "      reuse: whether or not the layer and its variables should be reused. To be\n",
      "        able to reuse the layer scope must be given.\n",
      "      variables_collections: optional collections for the variables.\n",
      "      outputs_collections: collections to add the outputs.\n",
      "      trainable: If `True` also add variables to the graph collection\n",
      "        `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n",
      "      batch_weights: An optional tensor of shape `[batch_size]`,\n",
      "        containing a frequency weight for each batch item. If present,\n",
      "        then the batch normalization uses weighted mean and\n",
      "        variance. (This can be used to correct for bias in training\n",
      "        example selection.)\n",
      "      fused:  Use nn.fused_batch_norm if True, nn.batch_normalization otherwise.\n",
      "      data_format: A string. `NHWC` (default) and `NCHW` are supported.\n",
      "      zero_debias_moving_mean: Use zero_debias for moving_mean. It creates a new\n",
      "        pair of variables 'moving_mean/biased' and 'moving_mean/local_step'.\n",
      "      scope: Optional scope for `variable_scope`.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` representing the output of the operation.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: if `batch_weights` is not None and `fused` is True.\n",
      "      ValueError: if `data_format` is neither `NHWC` nor `NCHW`.\n",
      "      ValueError: if the rank of `inputs` is undefined.\n",
      "      ValueError: if rank or channels dimension of `inputs` is undefined.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf.contrib.layers.batch_norm\n",
    "\n",
    "# related operation\n",
    "# tf.nn.batch_normalization\n",
    "\n",
    "help(tf.contrib.layers.batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function layer_norm in module tensorflow.contrib.layers.python.layers.layers:\n",
      "\n",
      "layer_norm(inputs, center=True, scale=True, activation_fn=None, reuse=None, variables_collections=None, outputs_collections=None, trainable=True, scope=None)\n",
      "    Adds a Layer Normalization layer from https://arxiv.org/abs/1607.06450.\n",
      "    \n",
      "      \"Layer Normalization\"\n",
      "    \n",
      "      Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton\n",
      "    \n",
      "    Can be used as a normalizer function for conv2d and fully_connected.\n",
      "    \n",
      "    Args:\n",
      "      inputs: a tensor with 2 or more dimensions. The normalization\n",
      "              occurs over all but the first dimension.\n",
      "      center: If True, add offset of `beta` to normalized tensor. If False, `beta`\n",
      "        is ignored.\n",
      "      scale: If True, multiply by `gamma`. If False, `gamma` is\n",
      "        not used. When the next layer is linear (also e.g. `nn.relu`), this can be\n",
      "        disabled since the scaling can be done by the next layer.\n",
      "      activation_fn: activation function, default set to None to skip it and\n",
      "        maintain a linear activation.\n",
      "      reuse: whether or not the layer and its variables should be reused. To be\n",
      "        able to reuse the layer scope must be given.\n",
      "      variables_collections: optional collections for the variables.\n",
      "      outputs_collections: collections to add the outputs.\n",
      "      trainable: If `True` also add variables to the graph collection\n",
      "        `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n",
      "      scope: Optional scope for `variable_scope`.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` representing the output of the operation.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: if rank or last dimension of `inputs` is undefined.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf.contrib.layers.layer_norm\n",
    "\n",
    "help(tf.contrib.layers.layer_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = tf.constant([[0,1], [1,2], [2,3]])\n",
    "\n",
    "g.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pad in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "pad(tensor, paddings, mode='CONSTANT', name=None)\n",
      "    Pads a tensor.\n",
      "    \n",
      "    This operation pads a `tensor` according to the `paddings` you specify.\n",
      "    `paddings` is an integer tensor with shape `[n, 2]`, where n is the rank of\n",
      "    `tensor`. For each dimension D of `input`, `paddings[D, 0]` indicates how\n",
      "    many values to add before the contents of `tensor` in that dimension, and\n",
      "    `paddings[D, 1]` indicates how many values to add after the contents of\n",
      "    `tensor` in that dimension. If `mode` is \"REFLECT\" then both `paddings[D, 0]`\n",
      "    and `paddings[D, 1]` must be no greater than `tensor.dim_size(D) - 1`. If\n",
      "    `mode` is \"SYMMETRIC\" then both `paddings[D, 0]` and `paddings[D, 1]` must be\n",
      "    no greater than `tensor.dim_size(D)`.\n",
      "    \n",
      "    The padded size of each dimension D of the output is:\n",
      "    \n",
      "    `paddings[D, 0] + tensor.dim_size(D) + paddings[D, 1]`\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    # 't' is [[1, 2, 3], [4, 5, 6]].\n",
      "    # 'paddings' is [[1, 1,], [2, 2]].\n",
      "    # rank of 't' is 2.\n",
      "    pad(t, paddings, \"CONSTANT\") ==> [[0, 0, 0, 0, 0, 0, 0],\n",
      "                                      [0, 0, 1, 2, 3, 0, 0],\n",
      "                                      [0, 0, 4, 5, 6, 0, 0],\n",
      "                                      [0, 0, 0, 0, 0, 0, 0]]\n",
      "    \n",
      "    pad(t, paddings, \"REFLECT\") ==> [[6, 5, 4, 5, 6, 5, 4],\n",
      "                                     [3, 2, 1, 2, 3, 2, 1],\n",
      "                                     [6, 5, 4, 5, 6, 5, 4],\n",
      "                                     [3, 2, 1, 2, 3, 2, 1]]\n",
      "    \n",
      "    pad(t, paddings, \"SYMMETRIC\") ==> [[2, 1, 1, 2, 3, 3, 2],\n",
      "                                       [2, 1, 1, 2, 3, 3, 2],\n",
      "                                       [5, 4, 4, 5, 6, 6, 5],\n",
      "                                       [5, 4, 4, 5, 6, 6, 5]]\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      tensor: A `Tensor`.\n",
      "      paddings: A `Tensor` of type `int32`.\n",
      "      mode: One of \"CONSTANT\", \"REFLECT\", or \"SYMMETRIC\" (case-insensitive)\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `tensor`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: When mode is not one of \"CONSTANT\", \"REFLECT\", or \"SYMMETRIC\".\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 1,  2,  3],\n",
       "        [ 4,  5,  6],\n",
       "        [ 7,  8,  9],\n",
       "        [10, 11, 12],\n",
       "        [13, 14, 15]],\n",
       "\n",
       "       [[ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 1,  2,  3],\n",
       "        [ 4,  5,  6],\n",
       "        [ 7,  8,  9],\n",
       "        [10, 11, 12],\n",
       "        [13, 14, 15]]], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.pad\n",
    "\n",
    "help(tf.pad)\n",
    "\n",
    "pad_len = 5\n",
    "\n",
    "inputs = tf.constant([\n",
    "    [\n",
    "        [1,2,3],\n",
    "        [4,5,6],\n",
    "        [7,8,9],\n",
    "        [10,11,12],\n",
    "        [13,14,15]\n",
    "    ],\n",
    "    [\n",
    "        [1,2,3],\n",
    "        [4,5,6],\n",
    "        [7,8,9],\n",
    "        [10,11,12],\n",
    "        [13,14,15]\n",
    "    ]\n",
    "])\n",
    "\n",
    "g = tf.pad(inputs, [[0,0], [pad_len, 0], [0,0]])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    a = sess.run(g)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function conv1d in module tensorflow.python.layers.convolutional:\n",
      "\n",
      "conv1d(inputs, filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, activation=None, use_bias=True, kernel_initializer=None, bias_initializer=<tensorflow.python.ops.init_ops.Zeros object at 0x114976748>, kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, trainable=True, name=None, reuse=None)\n",
      "    Functional interface for 1D convolution layer (e.g. temporal convolution).\n",
      "    \n",
      "    This layer creates a convolution kernel that is convolved\n",
      "    (actually cross-correlated) with the layer input to produce a tensor of\n",
      "    outputs. If `use_bias` is True (and a `bias_initializer` is provided),\n",
      "    a bias vector is created and added to the outputs. Finally, if\n",
      "    `activation` is not `None`, it is applied to the outputs as well.\n",
      "    \n",
      "    Arguments:\n",
      "      inputs: Tensor input.\n",
      "      filters: integer, the dimensionality of the output space (i.e. the number\n",
      "        output of filters in the convolution).\n",
      "      kernel_size: An integer or tuple/list of a single integer, specifying the\n",
      "        length of the 1D convolution window.\n",
      "      strides: an integer or tuple/list of a single integer,\n",
      "        specifying the stride length of the convolution.\n",
      "        Specifying any stride value != 1 is incompatible with specifying\n",
      "        any `dilation_rate` value != 1.\n",
      "      padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n",
      "      data_format: A string, one of `channels_last` (default) or `channels_first`.\n",
      "        The ordering of the dimensions in the inputs.\n",
      "        `channels_last` corresponds to inputs with shape\n",
      "        `(batch, length, channels)` while `channels_first` corresponds to\n",
      "        inputs with shape `(batch, channels, length)`.\n",
      "      dilation_rate: an integer or tuple/list of a single integer, specifying\n",
      "        the dilation rate to use for dilated convolution.\n",
      "        Currently, specifying any `dilation_rate` value != 1 is\n",
      "        incompatible with specifying any `strides` value != 1.\n",
      "      activation: Activation function. Set it to None to maintain a\n",
      "        linear activation.\n",
      "      use_bias: Boolean, whether the layer uses a bias.\n",
      "      kernel_initializer: An initializer for the convolution kernel.\n",
      "      bias_initializer: An initializer for the bias vector. If None, no bias will\n",
      "        be applied.\n",
      "      kernel_regularizer: Optional regularizer for the convolution kernel.\n",
      "      bias_regularizer: Optional regularizer for the bias vector.\n",
      "      activity_regularizer: Regularizer function for the output.\n",
      "      trainable: Boolean, if `True` also add variables to the graph collection\n",
      "        `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n",
      "      name: A string, the name of the layer.\n",
      "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
      "        by the same name.\n",
      "    \n",
      "    Returns:\n",
      "      Output tensor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 18.],\n",
       "        [ 18.]]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.conv1d\n",
    "\n",
    "help(tf.layers.conv1d)\n",
    "\n",
    "inputs = tf.constant([\n",
    "    [\n",
    "    [1,2,3,0],\n",
    "    [0,1,2,3],\n",
    "    [3,0,1,2],\n",
    "    [2,3,0,1]\n",
    "    ]\n",
    "], dtype=tf.float32)\n",
    "\n",
    "filters = 1\n",
    "kernel_size = 3\n",
    "\n",
    "kernel_initializer = tf.constant_initializer(1.)\n",
    "\n",
    "g = tf.layers.conv1d(inputs=inputs,\n",
    "                     filters=filters,\n",
    "                     kernel_size=kernel_size,\n",
    "                     kernel_initializer=kernel_initializer,\n",
    "                     use_bias=False)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    a = sess.run(g)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BahdanauAttention in module tensorflow.contrib.seq2seq.python.ops.attention_wrapper:\n",
      "\n",
      "class BahdanauAttention(_BaseAttentionMechanism)\n",
      " |  Implements Bhadanau-style (additive) attention.\n",
      " |  \n",
      " |  This attention has two forms.  The first is Bhandanau attention,\n",
      " |  as described in:\n",
      " |  \n",
      " |  Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.\n",
      " |  \"Neural Machine Translation by Jointly Learning to Align and Translate.\"\n",
      " |  ICLR 2015. https://arxiv.org/abs/1409.0473\n",
      " |  \n",
      " |  The second is the normalized form.  This form is inspired by the\n",
      " |  weight normalization article:\n",
      " |  \n",
      " |  Tim Salimans, Diederik P. Kingma.\n",
      " |  \"Weight Normalization: A Simple Reparameterization to Accelerate\n",
      " |   Training of Deep Neural Networks.\"\n",
      " |  https://arxiv.org/abs/1602.07868\n",
      " |  \n",
      " |  To enable the second form, construct the object with parameter\n",
      " |  `normalize=True`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BahdanauAttention\n",
      " |      _BaseAttentionMechanism\n",
      " |      AttentionMechanism\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, query, previous_alignments)\n",
      " |      Score the query based on the keys and values.\n",
      " |      \n",
      " |      Args:\n",
      " |        query: Tensor of dtype matching `self.values` and shape\n",
      " |          `[batch_size, query_depth]`.\n",
      " |        previous_alignments: Tensor of dtype matching `self.values` and shape\n",
      " |          `[batch_size, alignments_size]`\n",
      " |          (`alignments_size` is memory's `max_time`).\n",
      " |      \n",
      " |      Returns:\n",
      " |        alignments: Tensor of dtype matching `self.values` and shape\n",
      " |          `[batch_size, alignments_size]` (`alignments_size` is memory's\n",
      " |          `max_time`).\n",
      " |  \n",
      " |  __init__(self, num_units, memory, memory_sequence_length=None, normalize=False, probability_fn=None, score_mask_value=-inf, name='BahdanauAttention')\n",
      " |      Construct the Attention mechanism.\n",
      " |      \n",
      " |      Args:\n",
      " |        num_units: The depth of the query mechanism.\n",
      " |        memory: The memory to query; usually the output of an RNN encoder.  This\n",
      " |          tensor should be shaped `[batch_size, max_time, ...]`.\n",
      " |        memory_sequence_length (optional): Sequence lengths for the batch entries\n",
      " |          in memory.  If provided, the memory tensor rows are masked with zeros\n",
      " |          for values past the respective sequence lengths.\n",
      " |        normalize: Python boolean.  Whether to normalize the energy term.\n",
      " |        probability_fn: (optional) A `callable`.  Converts the score to\n",
      " |          probabilities.  The default is @{tf.nn.softmax}. Other options include\n",
      " |          @{tf.contrib.seq2seq.hardmax} and @{tf.contrib.sparsemax.sparsemax}.\n",
      " |          Its signature should be: `probabilities = probability_fn(score)`.\n",
      " |        score_mask_value: (optional): The mask value for score before passing into\n",
      " |          `probability_fn`. The default is -inf. Only used if\n",
      " |          `memory_sequence_length` is not None.\n",
      " |        name: Name to use when creating ops.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseAttentionMechanism:\n",
      " |  \n",
      " |  initial_alignments(self, batch_size, dtype)\n",
      " |      Creates the initial alignment values for the `AttentionWrapper` class.\n",
      " |      \n",
      " |      This is important for AttentionMechanisms that use the previous alignment\n",
      " |      to calculate the alignment at the next time step (e.g. monotonic attention).\n",
      " |      \n",
      " |      The default behavior is to return a tensor of all zeros.\n",
      " |      \n",
      " |      Args:\n",
      " |        batch_size: `int32` scalar, the batch_size.\n",
      " |        dtype: The `dtype`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `dtype` tensor shaped `[batch_size, alignments_size]`\n",
      " |        (`alignments_size` is the values' `max_time`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _BaseAttentionMechanism:\n",
      " |  \n",
      " |  alignments_size\n",
      " |  \n",
      " |  batch_size\n",
      " |  \n",
      " |  keys\n",
      " |  \n",
      " |  memory_layer\n",
      " |  \n",
      " |  query_layer\n",
      " |  \n",
      " |  values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from AttentionMechanism:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf.contrib.seq2seq.BahdanauAttention\n",
    "\n",
    "help(tf.contrib.seq2seq.BahdanauAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class AttentionWrapper in module tensorflow.contrib.seq2seq.python.ops.attention_wrapper:\n",
      "\n",
      "class AttentionWrapper(tensorflow.python.ops.rnn_cell_impl.RNNCell)\n",
      " |  Wraps another `RNNCell` with attention.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      AttentionWrapper\n",
      " |      tensorflow.python.ops.rnn_cell_impl.RNNCell\n",
      " |      tensorflow.python.layers.base.Layer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, cell, attention_mechanism, attention_layer_size=None, alignment_history=False, cell_input_fn=None, output_attention=True, initial_cell_state=None, name=None)\n",
      " |      Construct the `AttentionWrapper`.\n",
      " |      \n",
      " |      Args:\n",
      " |        cell: An instance of `RNNCell`.\n",
      " |        attention_mechanism: An instance of `AttentionMechanism`.\n",
      " |        attention_layer_size: Python integer, the depth of the attention (output)\n",
      " |          layer. If None (default), use the context as attention at each time\n",
      " |          step. Otherwise, feed the context and cell output into the attention\n",
      " |          layer to generate attention at each time step.\n",
      " |        alignment_history: Python boolean, whether to store alignment history\n",
      " |          from all time steps in the final output state (currently stored as a\n",
      " |          time major `TensorArray` on which you must call `stack()`).\n",
      " |        cell_input_fn: (optional) A `callable`.  The default is:\n",
      " |          `lambda inputs, attention: array_ops.concat([inputs, attention], -1)`.\n",
      " |        output_attention: Python bool.  If `True` (default), the output at each\n",
      " |          time step is the attention value.  This is the behavior of Luong-style\n",
      " |          attention mechanisms.  If `False`, the output at each time step is\n",
      " |          the output of `cell`.  This is the beahvior of Bhadanau-style\n",
      " |          attention mechanisms.  In both cases, the `attention` tensor is\n",
      " |          propagated to the next time step via the state and is used there.\n",
      " |          This flag only controls whether the attention mechanism is propagated\n",
      " |          up to the next cell in an RNN stack or to the top RNN output.\n",
      " |        initial_cell_state: The initial state value to use for the cell when\n",
      " |          the user calls `zero_state()`.  Note that if this value is provided\n",
      " |          now, and the user uses a `batch_size` argument of `zero_state` which\n",
      " |          does not match the batch size of `initial_cell_state`, proper\n",
      " |          behavior is not guaranteed.\n",
      " |        name: Name to use when creating ops.\n",
      " |  \n",
      " |  call(self, inputs, state)\n",
      " |      Perform a step of attention-wrapped RNN.\n",
      " |      \n",
      " |      - Step 1: Mix the `inputs` and previous step's `attention` output via\n",
      " |        `cell_input_fn`.\n",
      " |      - Step 2: Call the wrapped `cell` with this input and its previous state.\n",
      " |      - Step 3: Score the cell's output with `attention_mechanism`.\n",
      " |      - Step 4: Calculate the alignments by passing the score through the\n",
      " |        `normalizer`.\n",
      " |      - Step 5: Calculate the context vector as the inner product between the\n",
      " |        alignments and the attention_mechanism's values (memory).\n",
      " |      - Step 6: Calculate the attention output by concatenating the cell output\n",
      " |        and context through the attention layer (a linear layer with\n",
      " |        `attention_size` outputs).\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: (Possibly nested tuple of) Tensor, the input at this time step.\n",
      " |        state: An instance of `AttentionWrapperState` containing\n",
      " |          tensors from the previous time step.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tuple `(attention_or_cell_output, next_state)`, where:\n",
      " |      \n",
      " |        - `attention_or_cell_output` depending on `output_attention`.\n",
      " |        - `next_state` is an instance of `DynamicAttentionWrapperState`\n",
      " |           containing the state calculated at this time step.\n",
      " |  \n",
      " |  zero_state(self, batch_size, dtype)\n",
      " |      Return zero-filled state tensor(s).\n",
      " |      \n",
      " |      Args:\n",
      " |        batch_size: int, float, or unit Tensor representing the batch size.\n",
      " |        dtype: the data type to use for the state.\n",
      " |      \n",
      " |      Returns:\n",
      " |        If `state_size` is an int or TensorShape, then the return value is a\n",
      " |        `N-D` tensor of shape `[batch_size x state_size]` filled with zeros.\n",
      " |      \n",
      " |        If `state_size` is a nested list or tuple, then the return value is\n",
      " |        a nested list or tuple (of the same structure) of `2-D` tensors with\n",
      " |        the shapes `[batch_size x s]` for each s in `state_size`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  output_size\n",
      " |      Integer or TensorShape: size of outputs produced by this cell.\n",
      " |  \n",
      " |  state_size\n",
      " |      size(s) of state(s) used by this cell.\n",
      " |      \n",
      " |      It can be represented by an Integer, a TensorShape or a tuple of Integers\n",
      " |      or TensorShapes.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.ops.rnn_cell_impl.RNNCell:\n",
      " |  \n",
      " |  __call__(self, inputs, state, scope=None)\n",
      " |      Run this RNN cell on inputs, starting from the given state.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: `2-D` tensor with shape `[batch_size x input_size]`.\n",
      " |        state: if `self.state_size` is an integer, this should be a `2-D Tensor`\n",
      " |          with shape `[batch_size x self.state_size]`.  Otherwise, if\n",
      " |          `self.state_size` is a tuple of integers, this should be a tuple\n",
      " |          with shapes `[batch_size x s] for s in self.state_size`.\n",
      " |        scope: VariableScope for the created subgraph; defaults to class name.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A pair containing:\n",
      " |      \n",
      " |        - Output: A `2-D` tensor with shape `[batch_size x self.output_size]`.\n",
      " |        - New state: Either a single `2-D` tensor, or a tuple of tensors matching\n",
      " |          the arity and shapes of `state`.\n",
      " |  \n",
      " |  build(self, _)\n",
      " |      Creates the variables of the layer.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.layers.base.Layer:\n",
      " |  \n",
      " |  __deepcopy__(self, memo)\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing a same layer\n",
      " |      on different inputs `a` and `b`, some entries in `layer.losses` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      The `get_losses_for` method allows to retrieve the losses relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors.\n",
      " |        inputs: Optional input tensor(s) that the loss(es) depend on. Must\n",
      " |          match the `inputs` argument passed to the `__call__` method at the time\n",
      " |          the losses are created. If `None` is passed, the losses are assumed\n",
      " |          to be unconditional, and will apply across all dataflows of the layer\n",
      " |          (e.g. weight regularization losses).\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing a same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      The `get_updates_for` method allows to retrieve the updates relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops.\n",
      " |        inputs: Optional input tensor(s) that the update(s) depend on. Must\n",
      " |          match the `inputs` argument passed to the `__call__` method at the time\n",
      " |          the updates are created. If `None` is passed, the updates are assumed\n",
      " |          to be unconditional, and will apply across all dataflows of the layer.\n",
      " |  \n",
      " |  add_variable(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True)\n",
      " |      Adds a new variable to the layer, or gets an existing one; returns it.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: variable name.\n",
      " |        shape: variable shape.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
      " |        initializer: initializer instance (callable).\n",
      " |        regularizer: regularizer instance (callable).\n",
      " |        trainable: whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean, stddev).\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Apply the layer on a input.\n",
      " |      \n",
      " |      This simply wraps `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |          Must match the `inputs` argument passed to the `__call__`\n",
      " |          method at the time the losses were created.\n",
      " |          If you pass `inputs=None`, unconditional losses are returned,\n",
      " |          such as weight regularization losses.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |          Must match the `inputs` argument passed to the `__call__` method\n",
      " |          at the time the updates were created.\n",
      " |          If you pass `inputs=None`, unconditional updates are returned.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.layers.base.Layer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  graph\n",
      " |  \n",
      " |  losses\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  scope_name\n",
      " |  \n",
      " |  trainable_variables\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf.contrib.seq2seq.AttentionWrapper\n",
    "\n",
    "help(tf.contrib.seq2seq.AttentionWrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function dense in module tensorflow.python.layers.core:\n",
      "\n",
      "dense(inputs, units, activation=None, use_bias=True, kernel_initializer=None, bias_initializer=<tensorflow.python.ops.init_ops.Zeros object at 0x118c940b8>, kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, trainable=True, name=None, reuse=None)\n",
      "    Functional interface for the densely-connected layer.\n",
      "    \n",
      "    This layer implements the operation:\n",
      "    `outputs = activation(inputs.kernel + bias)`\n",
      "    Where `activation` is the activation function passed as the `activation`\n",
      "    argument (if not `None`), `kernel` is a weights matrix created by the layer,\n",
      "    and `bias` is a bias vector created by the layer\n",
      "    (only if `use_bias` is `True`).\n",
      "    \n",
      "    Note: if the `inputs` tensor has a rank greater than 2, then it is\n",
      "    flattened prior to the initial matrix multiply by `kernel`.\n",
      "    \n",
      "    Arguments:\n",
      "      inputs: Tensor input.\n",
      "      units: Integer or Long, dimensionality of the output space.\n",
      "      activation: Activation function (callable). Set it to None to maintain a\n",
      "        linear activation.\n",
      "      use_bias: Boolean, whether the layer uses a bias.\n",
      "      kernel_initializer: Initializer function for the weight matrix.\n",
      "      bias_initializer: Initializer function for the bias.\n",
      "      kernel_regularizer: Regularizer function for the weight matrix.\n",
      "      bias_regularizer: Regularizer function for the bias.\n",
      "      activity_regularizer: Regularizer function for the output.\n",
      "      trainable: Boolean, if `True` also add variables to the graph collection\n",
      "        `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n",
      "      name: String, the name of the layer.\n",
      "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
      "        by the same name.\n",
      "    \n",
      "    Returns:\n",
      "      Output tensor.\n",
      "\n",
      "[[ 5]\n",
      " [11]]\n",
      "[ 5 11]\n",
      "\n",
      "[[ 3  6]\n",
      " [ 7 14]]\n",
      "[[ 3  6]\n",
      " [ 7 14]]\n"
     ]
    }
   ],
   "source": [
    "# tf.layers.dense\n",
    "\n",
    "help(tf.layers.dense)\n",
    "\n",
    "inputs1 = tf.constant([[1,2], [3,4]])\n",
    "inputs2 = tf.constant([[1,2], [3,4]])\n",
    "\n",
    "g = tf.layers.dense(inputs1, units=1, activation=None, kernel_initializer=tf.constant_initializer([1,2]))\n",
    "f = tf.layers.dense(inputs2, units=2, activation=None, kernel_initializer=tf.constant_initializer([[1,2], [1,2]]))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    gv= sess.run(g)\n",
    "    fv = sess.run(f)\n",
    "\n",
    "a = np.array([[1,2], [3,4]])\n",
    "b = np.array([1,2])\n",
    "gva = np.matmul(a, b)    \n",
    "\n",
    "c = np.array([[1,2], [3,4]])\n",
    "d = np.array([[1,2], [1,2]])\n",
    "fva = np.matmul(c, d)\n",
    "    \n",
    "print(gv)\n",
    "print(gva)\n",
    "print()\n",
    "\n",
    "print(fv)\n",
    "print(fva)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo:\n",
    "\n",
    "- [ ] tf.layers.conv1d\n",
    "- [ ] tf.contrib.rnn.GRUCell\n",
    "- [ ] tf.nn.bidirectional_dynamic_rnn\n",
    "- [ ] tf.nn.dynamic_rnn\n",
    "- [ ] tf.contrib.seq2seq.BahdanauAttention\n",
    "- [ ] tf.contrib.seq2seq.AttentionWrapper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
