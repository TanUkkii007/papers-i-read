{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation by Jointly Learning to Align and Translate\n",
    "\n",
    "Link: https://arxiv.org/abs/1409.0473\n",
    "\n",
    "Authors: Dzmitry Bahdanau, KyungHyun Cho, Yoshua Bengio\n",
    "\n",
    "Institution: Jacobs University \n",
    "\n",
    "Publication: Universite ́ de Montre ́al\n",
    "\n",
    "Date: 19 May 2016 (as of v7)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Materials\n",
    "\n",
    "- Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation https://arxiv.org/abs/1406.1078\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Papers citing this paper\n",
    "\n",
    "- Tacotron: Towards End-to-End Speech Synthesis https://arxiv.org/abs/1703.10135\n",
    "- and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this paper about?\n",
    "\n",
    "An encode-decoder architecture for neural machine translation that is not constraind to fixed-length vector representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the motivation of this research?\n",
    "\n",
    "Cho et al., 2014a proposed encode-decoder architecture for machine translation. The encoder encode source sentence inputs into fixed-length vector and the decoder outputs a translation from the encoded vector. \n",
    "\n",
    "Cho et al., 2014b showed that the performance of a encode-decoder model deteriorates as the length of an input sequence increases.\n",
    "\n",
    "They conjecture that use of the fixed-length vector is a bottleneck to improve performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What makes this paper different from previous research?\n",
    "\n",
    "They extended the encoder-decoder model so that it can soft-serarch for a set of positions in a source sentence where the most relevant infomation is concentrated.\n",
    "\n",
    "The proposed model encodes an input sentence into a sequence of vectors and choose a subset of these vectors adaptively while decoding the translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this paper achieve it?\n",
    "\n",
    "### Decoder\n",
    "\n",
    "The decoder is often trained to predict the next word $y_t$ given the context vector $c$ and all the previously predicted words $\\{y_1, ..., y_{t-1}\\}$. \n",
    "\n",
    "$p(\\boldsymbol{y}) = p(\\{y_1,...,y_T\\}) = \\prod_{t=1}^Tp(y_t \\lvert \\{y_1,...,y_{t-1}\\}, c)$\n",
    "\n",
    "With RNN, each conditional probability is modeled as \n",
    "\n",
    "$p(y_t \\lvert \\{y_1, ..., y_{t-1}\\}, c) = g(y_{t-1}, s_t, c)$\n",
    "\n",
    "where $g$ is nonlinear, potentially multi-layered function that outputs the probability of $y_t$, and $s_t$ is the hidden state of the RNN.\n",
    "\n",
    "In the proposed model, the each conditional probability above is defined as \n",
    "\n",
    "$p(y_i\\lvert y_1,...,y_{i-1}, \\boldsymbol{x}) = g(y_{i-1}, s_t, c_i)$\n",
    "\n",
    "where $s_i$ is an RNN hidden state for time $i$ computed by\n",
    "\n",
    "$s_i = f(s_{i-1}, y_{i-1}, c_i)$\n",
    "\n",
    "The context vector $c_i$ depends on a sequence of *annotations* $(h_1,...,h_T)$ to which an encoder maps the input sentence. The next section about an encoder explains how to compute the annotations.\n",
    "\n",
    "The context vector $c_i$ is then computed as a weighted sum of the annotations.\n",
    "\n",
    "$c_i = \\sum_{j=1}^T\\alpha_{ij}h_j$\n",
    "\n",
    "The weight $\\alpha_{ij}$ of each annotation h_j is computed by\n",
    "\n",
    "$\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{K=1}^T\\exp(e_{ik})}$\n",
    "\n",
    "where \n",
    "\n",
    "$e_{ij} = a(s_{i-1}, h_j)$\n",
    "\n",
    "is an *alignment model* which scores how the inputs around position $j$ and the output position $i$ match.\n",
    "\n",
    "\n",
    "### Encoder\n",
    "\n",
    "For annotations to summarize not only the preceeding words but also the following words, bidirectional RNN is used.\n",
    "\n",
    "The backward RNN reads the sequence in the reverse order.\n",
    "\n",
    "An annotation for each word $x_j$ is obtained by concatenating the forward hidden state $\\overrightarrow{h_j}$ and the backward one $\\overleftarrow{h_j}$,\n",
    "\n",
    "$h_j = [\\overrightarrow{h_j}^\\top ; \\overleftarrow{h_j}^\\top]$\n",
    "\n",
    "<img src=\"img/Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate_Figure1.png\" width=\"200\">\n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "<img src=\"img/Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate_Figure2.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset used in this study\n",
    "\n",
    "- WMT' 14: English-French parallel corpora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations\n",
    "\n",
    "- tf.contrib.seq2seq.BahdanauAttention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Readings\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
