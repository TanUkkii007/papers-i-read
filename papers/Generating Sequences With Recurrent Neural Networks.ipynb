{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Sequences With Recurrent Neural Networks\n",
    "\n",
    "Link: https://arxiv.org/abs/1308.0850\n",
    "\n",
    "Authors: Alex Graves\n",
    "\n",
    "Institution: Department of Computer Science University of Toronto\n",
    "\n",
    "Publication: arXiv\n",
    "\n",
    "Date: 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Materials\n",
    "\n",
    "- LSTM implementation explained https://apaszke.github.io/lstm-explained.html\n",
    "- Understanding LSTM Networks http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this paper about?\n",
    "\n",
    "How LSTM can generate sequences with long-range structure such as text and online handwriting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the motivation of this research?\n",
    "\n",
    "To demonstrate LSTM can use its memory to generate complex sequences containig long-range structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What makes this paper different from previous research?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this paper achieve it?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN formulation\n",
    "\n",
    "<img src=\"./img/Generating Sequences With Recurrent Neural Networks Figure1.png\" width=\"500\">\n",
    "\n",
    "The hidden layer activations are iterations of following equations,\n",
    "\n",
    "$h_t^1 = \\mathcal{H}(W_{\\mathrm{input\\times h^1}x_t} + W_{h^1\\times h^1} + b_h^1)$  ($n = 1$)\n",
    "\n",
    "$h_t^n = \\mathcal{H}(W_{\\mathrm{input}\\times h^n x_t} + W_{h^{n-1}\\times h^n} + W_{h^n\\times h^n}h_{t-1}^n + b_h^n)$  ($2 \\le n \\le N$)\n",
    "\n",
    "where $\\mathcal{H}$ is an activation function, typically sigmoid. \n",
    "\n",
    "The probability of input sequence $x$ is \n",
    "\n",
    "$\\mathrm{Pr}(x) = \\prod_{t=1}^T\\mathrm{Pr}(x_{t+1}|y_t)$\n",
    "\n",
    "where $x = (x_1,...,x_T)$ is input vector sequence and $y = (y_1,...,y_T)$ is output vector sequence. \n",
    "\n",
    "The sequence loss function is negative logarithm of $\\mathrm{Pr}$.\n",
    "\n",
    "$\\mathcal{L}(x) = -\\sum_{t=1}^T\\log\\mathrm{Pr}(x_{t+1}|y_t)$\n",
    "\n",
    "\n",
    "### Long Short-Term Memory formulation\n",
    "\n",
    "While hidden layer function $\\mathcal{H}$ of most RNN is an elementwise application of a sigmoid function, LSTM uses memory cells formulated as follows.\n",
    "\n",
    "<img src=\"./img/Generating Sequences With Recurrent Neural Networks Figure2.png\" width=\"500\">\n",
    "\n",
    "$h_t = o_t\\tanh(c_t)$\n",
    "\n",
    "$o_t = \\sigma(W_{xo}x_t + W_{ho}h_{t-1} + W_{co}c_t + b_o)$\n",
    "\n",
    "$c_t = f_tc_{t-1} + i_t\\tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c)$\n",
    "\n",
    "$f_t = \\sigma(W_{xf}x_t + W_{hf}h_{t-1} + W_{cf}c_{t-1} + b_f)$\n",
    "\n",
    "$i_t = \\sigma(W_{xi}x_t + W_{hi}h_{t-1} + W_{ci}c_{t-1} + b_i)$\n",
    "\n",
    "where $\\sigma$ is logistic sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Prediction\n",
    "\n",
    "Text data is discrete. The probability that text $x_t$ at time $t$ is $k$ is classification problem of $K$ classes if there is $K$ text classes in total.\n",
    "\n",
    "$\\mathrm{Pr}(x_{t+1} = k|y_t) = y_t^k = \\frac{\\exp(\\hat{y}_t^k)}{\\sum_{k=1}^K\\exp(\\hat{y}_t^k)}$\n",
    "\n",
    "where $y_t = \\mathcal{Y}(\\hat{y_t})$ and $\\mathcal{Y}$ is output layer function, here it is softmax function.\n",
    "\n",
    "From the following equation,\n",
    "\n",
    "$\\mathcal{L}(x) = -\\sum_{t=1}^T\\log\\mathrm{Pr}(x_{t+1}|y_t)$\n",
    "\n",
    "the loss function is\n",
    "\n",
    "$\\mathcal{L}(x) = -\\sum_{t=1}^T\\log y_t^{x_{t+1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Prediction Results\n",
    "\n",
    "In Penn Treebank experiments,\n",
    "\n",
    "- two equivalent metrics is used\n",
    " - bits-per-character (BPC): average value of $-\\log_2\\mathrm{Pr}(x_{t+1}|y_t)$\n",
    " - perplexity: $2^{average-character-length\\times \\mathrm{BPC}}$\n",
    "- word-level RNN performed better than the character- level network\n",
    "- comparable to Mikolov (2012) but benefit of dynamic evaluation(*) is more pronounced in LSTM than RNN, which suggests rapidly adapting to new data.\n",
    "\n",
    "(*) Dynamic evaluation is to allow a network to adapt its weight while is beeing evaluated on test data. It is legitimate for prediction problem so long as it only sees the test data once.\n",
    "\n",
    "\n",
    "In Wikipedia experiments,\n",
    "\n",
    "- character level RNN achieved 1.54 BPC and improved to 1.47 with maximum entropy model\n",
    " - for comparison the Hutter Prize (*) winner variant of PAQ-8 algorithm achieves 1.28 BPC. Mainstream compressors such as zip get more than 2.\n",
    "- See generated samples by the prediction network included in this paper\n",
    " \n",
    "(*) Hutter Prize is a challenge to compress the first 100 million bytes of the complete English Wikipedia data to as small as possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handwriting Prediction\n",
    "\n",
    "The idea of mixture density network is to use the output to parameterise a mixture distribution.\n",
    "Mixture density outputs can also be used with RNN. In this case the output distribution is conditioned on the history of inputs.\n",
    "\n",
    "The basic RNN architecture is unchanged.\n",
    "\n",
    "Each input vector $x_t$ consists of $(x_1, x_2)$ pair that is the pen offset from the previous input.\n",
    "In addition a binary $x_3$ represents end of stroke (1 means the pen is lifted and 0 for otherwise).\n",
    "\n",
    "$x_t \\in \\mathbb{R} \\times \\mathbb{R} \\times \\{0, 1\\}$\n",
    "\n",
    "A mixture of bivariate Gaussian was used to predict $x_1, x_2$ and Bernoulli distribution was used for $x_3$.\n",
    "\n",
    "Each output vector $y_t$ is\n",
    "\n",
    "$y_t = (e_t, \\{\\pi_t^j, \\mu_t^j, \\sigma_t^j, \\rho_t^j\\}_{j=1}^M)$\n",
    "\n",
    "where $e$ is the end of stroke probability, $\\mu^j$ is means, $\\sigma^j$ is standard deviations, $\\rho^j$ is collerations and $\\pi^j$ is mixture weights for $M$ components.\n",
    "\n",
    "The vectors $y_t$ are obtained from the network outputs $\\hat{y}_t$.\n",
    "\n",
    "$\\hat{y}_t = (\\hat{e}_t, \\{\\hat{w}_t^j, \\hat{\\mu}_t^j, \\hat{\\sigma}_t^j, \\hat{\\rho}_t^j\\}_{j=1}^M) = b_y + \\sum_{n=1}^NW_{h^ny}h_t^n$\n",
    "\n",
    "Each parameter is activated as follows:\n",
    "\n",
    "$e_t = \\mathrm{sigmoid}(\\hat{e}_t)$\n",
    "\n",
    "$\\pi_t^j = \\mathrm{softmax}(\\hat{\\pi}_t^j)$\n",
    "\n",
    "$\\mu_t^j = \\mathrm{identity}(\\hat{\\mu}_t^j)$\n",
    "\n",
    "$\\sigma_t^j = \\exp(\\hat{\\sigma}_t^j)$\n",
    "\n",
    "$\\rho_t^j = \\tanh(\\hat{\\rho}_t^j)$\n",
    "\n",
    "The probability density $Pr(x_{t+1}|y_t)$ is\n",
    "\n",
    "$\\mathrm{Pr}(x_{t+1}|y_t) = \\sum_{j=1}^M\\pi_t^j\\mathcal{N}(x_{t+1}|\\mu_t^j, \\sigma_t^j, \\rho_t^j)\\mathcal{B}((x_{t+1})_3;e_t)$\n",
    "\n",
    "where \n",
    "\n",
    "$\\mathcal{N}(x|\\mu, \\sigma, \\rho) = \\mathrm{Gaussian}(x| \\mu, \\sigma, \\rho) = \\frac{1}{2\\pi\\sigma_1\\sigma_2\\sqrt{1-\\rho^2}}\\exp[\\frac{-Z}{2(1-\\rho^2)}]$ \n",
    "\n",
    "and\n",
    "\n",
    "$\\mathcal{B}((x_{t+1})_3;e_t) = \\mathrm{Bernoulli}((x_{t+1})_3; e_t) = e_t^{(x_{t+1})_3}(1-e_t)^{1 - (x_{t+1})_3}$\n",
    "\n",
    "Note that $(x_{t+1})_3$ is 1 or 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handwriting Prediction Results\n",
    "\n",
    "In IAM online handwriting database experiments,\n",
    "\n",
    "- samples show strokes, letters and short words (e.g. \"of\" and \"the\")\n",
    "- generated handwrighting samples are included in this paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handwriting Synthesis\n",
    "\n",
    "Handwriting synthesis is the generation of handwriting for a given text.\n",
    "This can be done by conditioning a prediction network with annotation sequence (a character string).\n",
    "\n",
    "This problem is difficult because\n",
    "\n",
    "> The main challenge in conditioning the predictions on the text is that the two sequences are of very different lengths (the pen trace being on average twenty five times as long as the text), and the alignment between them is unknown until the data is generated. This is because the number of co-ordinates used to write each character varies greatly according to style, size, pen speed etc.\n",
    "\n",
    "<img src=\"./img/Generating Sequences With Recurrent Neural Networks Figure12.png\" width=\"500\">\n",
    "\n",
    "The difference of the network architectire is the added input from the character sequence mediated by the window layer.\n",
    "\n",
    "Given a length $U$ character sequence $c$ and a length $T$ data sequence $x$, the soft window $w_t$ at timestep $t$ $(1 \\le t \\le T)$ is defined as,\n",
    "\n",
    "$w_t = \\sum_{u=1}^U\\phi(t, u)c_u$\n",
    "\n",
    "where $\\phi(t, u)$ is the window weight of $c_u$ at timestep $t$.\n",
    "\n",
    "$\\phi(t, u) = \\sum_{k=1}^K\\alpha_t^k\\exp(-\\beta_t^k(\\kappa_t^k-u)^2)$\n",
    "\n",
    "> Intuitively, the $\\kappa_t$ parameters control the location of the window, the $\\beta_t$ parameters control the width of the window and the $\\alpha_t$ parameters control the importance of the window within the mixture.\n",
    "\n",
    "The equation for the output layer remain unchanged.\n",
    "\n",
    "The output probability is\n",
    "\n",
    "$\\mathrm{Pr}(x|c) = \\prod_{t=1}^T\\mathrm{Pr}(x_{t+1}|y_t)$\n",
    "\n",
    "The sequence loss is \n",
    "\n",
    "$\\mathcal{L}(x) = -\\log\\mathrm{Pr}(x|c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbiased Sampling Results\n",
    "\n",
    "- draw $x_{t+1}$ until $\\phi(t, U + 1) > \\phi(t, u) \\forall 1 \\le u \\le U$\n",
    "- generated samples are included in this paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biased Sampling Results\n",
    "\n",
    "Unbiased sampling generates handwritings that are not easy to read.\n",
    "\n",
    "To obtain easy-to-read handwritings, more probable prediction is selected by biasing.\n",
    "This is based on an expectation that good handwriting are more predictable than bad one.\n",
    "\n",
    "Define the probability bias $b$ as a real number greater than or equal to zero.\n",
    "\n",
    "Gaussian mixture is recalculated to\n",
    "\n",
    "$\\sigma_t^j = \\exp(\\hat{\\sigma}_t^j - b)$\n",
    "\n",
    "and each mixture weight is recalculated to\n",
    "\n",
    "$\\pi_t^j = \\mathrm{softmax}(\\hat{\\pi}_t^j(1 + b)) = \\frac{\\exp(\\hat{\\pi}_t^j(1 + b))}{\\sum_{j' = 1}^M\\exp(\\hat{\\pi}_t^{j'}(1 + b))}$\n",
    "\n",
    "This artificially reduces the variance.\n",
    "When $b = 0$ unbiased sampling is recovered.\n",
    "\n",
    "The generated samples are included in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primed Sampling Results\n",
    "\n",
    "Primed sampling generates handwriting in the style of a particular writer.\n",
    "\n",
    "This can be achieved by starting with a real sequence that is to be mimiced and then generating an extension.\n",
    "This works because the real sequence to be mimiced still exists in the network's memory.\n",
    "\n",
    "The generated samples are included in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset used in this study\n",
    "\n",
    "- Penn Treebank\n",
    "- Hutter Prize Wikipedia datasets\n",
    "- IAM Online Handwriting Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations\n",
    "\n",
    "- https://sourceforge.net/projects/rnnl/\n",
    "- https://github.com/szcom/rnnlib (GitHub fork)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Readings\n",
    "\n",
    "- https://www.cs.toronto.edu/~graves/gen_seq_rnn.pdf (Slides)\n",
    "- http://www.cs.toronto.edu/~graves/handwriting.html (Online Demo)\n",
    "- http://people.idsia.ch/~juergen/lstm/ (LSTM tutorial)\n",
    "- [Gradient-based learning algorithms for recurrent networks and their computational complexity](http://dl.acm.org/citation.cfm?id=201801) (backpropagation through time explained)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
