{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixel Recurrent Neural Networks\n",
    "\n",
    "\n",
    "Link: https://arxiv.org/abs/1601.06759\n",
    "\n",
    "Authors: Aa Ìˆron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu\n",
    "\n",
    "Institution: Google DeepMind\n",
    "\n",
    "Publication: arXiv\n",
    "\n",
    "Date: 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Materials\n",
    "\n",
    "- PixelRNN review from Magenta https://github.com/tensorflow/magenta/blob/master/magenta/reviews/pixelrnn.md\n",
    "- Deep Residual Learning for Image Recognition https://arxiv.org/abs/1512.03385"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this paper about?\n",
    "\n",
    "two-dimentional pixcel by pixcel image generation by RNN and CNN variant deep neural networks architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the motivation of this research?\n",
    "\n",
    "\n",
    "- building complex and expressive model that are also tractable and scalable is hard in generative modeling\n",
    "  - intractable approach: VAE that focuses on stochastic latent variable\n",
    "  - tractable approach: autoregressive models such as NADE\n",
    "    - but not expressive enough to model highly nonlinear and long-range correlations\n",
    "- Thesis & Bethge (2015) showed very promising results with two-dimensional RNN in grayscale images and textures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What makes this paper different from previous research?\n",
    "\n",
    "- architectual novelities: fast two-dimensional recurrent layers and effective use of residual connections\n",
    "- achieved log-likelihood score considerably better than the previous state of the art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this paper achieve it?\n",
    "\n",
    "\n",
    "### Model\n",
    "\n",
    "\n",
    "#### Generating an Image Pixel by Pixel\n",
    "\n",
    "A joint distribution $p(x)$ of image $x$ formed of $n \\times n$ pixels is :\n",
    "\n",
    "$p(x) = \\prod_{i=1}^{n^2}p(x_i\\lvert x_1,...,x_{i-1})$\n",
    "\n",
    "Each pixel $x_i$ is in turn determined three color channel values, RGB.\n",
    "\n",
    "$p(x_i\\lvert x_1,...,x_{i-1}) = p(x_i\\lvert \\boldsymbol{x}_{<i}) = p(x_i,R\\lvert \\boldsymbol{x}_{<i})p(x_i,G \\lvert \\boldsymbol{x}_{<i},R)p(x_i,B \\lvert \\boldsymbol{x}_{<i}, x_i,R, x_i,G)$\n",
    "\n",
    "Each of the colors is thus conditioned on the other channnels as well as on all the previous pixels.\n",
    "\n",
    "#### Pixels as Discrete Variables\n",
    "\n",
    "$p(x)$ is models as a discrete distribution. Each cannel variable $x_i,*$ takes 256 distinct values (where * is R , G or B).\n",
    "\n",
    "The discrete distribution has an advantage of being arbitrary multimodal without prior on the shape, as shown in softmax activation below.\n",
    "\n",
    "<img src=\"img/Pixel_Recurrent_Neural_Networks_Figure6.png\" width=300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel Recurrent Neural Networks\n",
    "\n",
    "\n",
    "#### Row LSTM\n",
    "\n",
    "Row LSTM processes image row by row from top to bottom.\n",
    "\n",
    "The input-to-state component is first computed for entire two dimentional input map using one-dimensional convolution of size $k \\times 1$.\n",
    "\n",
    "The convolution is masked to include valid context as figure below (kernel size = 3).\n",
    "\n",
    "<img src=\"img/Pixel_Recurrent_Neural_Networks_v2_Figure2c.png\" width=\"200\">\n",
    "\n",
    "The state-to-state component of the LSTM layer, the new hidden state $h_i$ and cell state $c_i$ are obtained as follows:\n",
    "\n",
    "$c_i = f_i \\odot c_{i-1} + i_i \\odot g_i $ (from LSTM definition)\n",
    "\n",
    "$h_i = o_i \\odot \\tanh(c_i) $ (from LSTM definition)\n",
    "\n",
    "$[o_i, f_i, i_i, g_i] = \\sigma(K^{ss} \\circledast h_{i-1} + K^{is} \\circledast x_i)$ (modified from LSTM definition)\n",
    "\n",
    "where $g_i$ is the content gate (a new input), $x_i$ is input map of row $i$ of size $h \\times n \\times 1$, $\\circledast$ represents the convolution operation and $\\odot$ the element wise multiplication. $K^{ss}$ and $K^{is}$ are kernel weights for state-to-state and input-to-state components. $\\sigma$ is activation function.\n",
    "\n",
    "Each step computes the new state for an entire row of the input map.\n",
    "\n",
    "\n",
    "#### Diagonal BiLSTM\n",
    "\n",
    "The Diagonal BiLSTM is able to capturfe entire available context. Each of the two directions of the layer scans the image from a corner to a opposite corner in diagonal fashion.\n",
    "\n",
    "<img src=\"img/Pixel_Recurrent_Neural_Networks_v2_Figure2r.png\" width=\"200\">\n",
    "\n",
    "To apply convolution along diagonal easily, input map is first skewed as described below.\n",
    "\n",
    "<img src=\"img/Pixel_Recurrent_Neural_Networks_Figure3.png\" width=\"300\">\n",
    "\n",
    "For each of the two directions, the input-to-state component is simply a $1\\times1$ convolution $K^{is}$.\n",
    "\n",
    "The state-to-state recurrent component is then computed with column-wise convolution K^{ss} with kernel of size $2 \\times 1$.\n",
    "\n",
    "The output is caluculated with equation same as Row LSTM.\n",
    "\n",
    "The output feature map is skewed back to $n \\times n$ map and the right output is shifted down by one row to prevent from seeing future pixels and added to the left output map.\n",
    "\n",
    "The Diagonal BiLSTM has an advantage that it uses minimal $2 \\times 1$ convolutional kernel. Larger kernel size is not helpful because Diagonal BiLSTM has already global receptive field.\n",
    "\n",
    "\n",
    "#### Residual Connections\n",
    "\n",
    "PixelRNN was trained up to 12 layers.\n",
    "\n",
    "To increase convergence speed residual connections (He et al, 2015) were used.\n",
    "\n",
    "\n",
    "#### Masked Convolution\n",
    "\n",
    "Two types of masks are used for PixelRNN.\n",
    "\n",
    "<img src=\"img/Pixel_Recurrent_Neural_Networks_Figure2r.png\" width=\"200\">\n",
    "\n",
    "Mask A is applied only to the first layer and restricts the connections to those neighboring pixels and to those colors in the current pixels.\n",
    "\n",
    "Mask B is applied to all the subsequent layers and relaxed the restriction of mask A by allowing the connection from a color to itself.\n",
    "\n",
    "\n",
    "#### PixelCNN\n",
    "\n",
    "The Row and Diagonal LSTM has potentially unbounded dependency range but this comes with computational cost.\n",
    "\n",
    "The PixelCNN use standard convolutional layers to capture bounded receptive field.\n",
    "\n",
    "Multiple convolutional layers are used to preserve the spatial resolution.\n",
    "\n",
    "\n",
    "#### Multi-Scale PixelRNN\n",
    "\n",
    "The Multi-Scale PixelRNN is composed of an unconditional PixelRNN and one or more conditional PixelRNNs.\n",
    "\n",
    "The unconditional network first generates a smaller $s \\times s$ image that is subsampled from the original image.\n",
    "The conditional network then takes the $s \\times s$ image as an additional input and generates a larger $n \\times n$ image.\n",
    "\n",
    "<img src=\"img/Pixel_Recurrent_Neural_Networks_Figure2c.png\" width=\"100\" >\n",
    "\n",
    "The conditional network is biased with an upsampled version of the small $s \\times s$ image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset used in this study\n",
    "\n",
    "- MNIST\n",
    "- CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations\n",
    "\n",
    "- https://github.com/carpedm20/pixel-rnn-tensorflow\n",
    "- https://github.com/openai/pixel-cnn (PixelCNN++)\n",
    "- https://github.com/PrajitR/fast-pixel-cnn (PixelCNN++)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Readings\n",
    "\n",
    "- Conditional Image Generation with PixelCNN Decoders https://arxiv.org/abs/1606.05328\n",
    "- PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications https://arxiv.org/abs/1701.05517"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
