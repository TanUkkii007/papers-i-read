{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveNet: A Generative Model for Raw Audio\n",
    "\n",
    "Link: https://arxiv.org/abs/1609.03499\n",
    "\n",
    "Authors: Aa ̈ron van den Oord, Karen Simonyan, Nal Kalchbrenner, Sander Dieleman, Oriol Vinyals, Andrew Senior, Heiga Zen, Alex Graves, Koray Kavukcuoglu\n",
    "\n",
    "Institution: Google DeepMind\n",
    "\n",
    "Publication: arXiv\n",
    "\n",
    "Date: 2016\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Materials\n",
    "\n",
    "- https://deepmind.com/blog/wavenet-generative-model-raw-audio/\n",
    "- Pixel Recurrent Neural Networks https://arxiv.org/abs/1601.06759\n",
    "- Conditional Image Generation with PixelCNN Decoders https://arxiv.org/abs/1606.05328\n",
    "- http://musyoku.github.io/2016/09/18/wavenet-a-generative-model-for-raw-audio/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this paper about?\n",
    "\n",
    "WaveNet, high quality sample-level generative model with DNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the motivation of this research?\n",
    "\n",
    "Recent neural autoregressive generative models such as PixelCNN are able to model distributions over thousands of random variables. This suggests that similar approaches can suceed in generating wideband raw audio waveforms, which are signals with very high temporal resolution, at least 16000 samples per second.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What makes this paper different from previous research?\n",
    "\n",
    "- can generate raw audio directly\n",
    "- generated speech signals has naluralness never before reported in TTS\n",
    "- new architectures based on dilated causal convolution to deal with long-range temporal dependencies\n",
    "- a single model can generate different voices when conditioned speaker identity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this paper achieve it?\n",
    "\n",
    "The joint probability of a waveform $\\boldsymbol{x} = \\{x_1,...,x_T\\}$ is\n",
    "\n",
    "$p(\\boldsymbol{x}) = \\prod_{t=1}^T p(x_t \\lvert x_1,..., x_{t-1})$\n",
    "\n",
    "\n",
    "### DILATED CAUSAL CONVOLUTIONS\n",
    "\n",
    "Causal convolution is used to constrain so that the prediction $p(x_{t+1}\\lvert x_1,..., x_t)$ cannot depend on any of the future timesteps $x_{t+1}, x_{t+2}, x_T$. For images, the equivalent of a causal convolution is a masked convolution as seen in PixelRNN.\n",
    "\n",
    "One of the problems of causal convolutions is that they require many layers, or large filters to increase the receptive field.\n",
    "\n",
    "A diliated convolution is convolution where the filter is applied over an area larger than its length by skipping input values with certain step. It is equivalent to a convolution with a larger filter derived from the original filter by dilating it with zero. This is similar to pooling or strided convolutions but the output has the same size as the input.\n",
    "\n",
    "Stacked diliated convolutions enable networks to have large receptive fields.\n",
    "\n",
    "<img src=\"img/WaveNet_A_Generative_Model_for_Raw_Audio_Figure3.png\">\n",
    "\n",
    "\n",
    "### SOFTMAX DISTRIBUTIONS\n",
    "\n",
    "van den Oord et al. (2016a) showed that softmax distribution tends to work better than continuous model such as GMM to model the conditional distribution $p(x_t \\lvert x_1,..., x_{t-1})$.\n",
    "\n",
    "Raw audio is tipically represented as a sequence of 16-bit, which means 65536 probabilities of all possible values. By using $\\mu$-law companding transformation, the data is quantized to 256 possible values.\n",
    "\n",
    "$f(x_t) = \\mathrm{sign}(x_t)\\frac{\\ln(1 + \\mu |x_t|)}{\\ln(1 + \\mu)}$\n",
    "\n",
    "where $-1 \\gt x_t \\gt 1$ and $\\mu = 255$.\n",
    "\n",
    "\n",
    "### GATED ACTIVATION UNITS\n",
    "\n",
    "As in PixelCNN, gated activation unit is used.\n",
    "\n",
    "$\\boldsymbol{z} = \\tanh(W_{f,k} \\cdot \\sigma(W_{g,k} \\ast x)) $\n",
    "\n",
    "where $\\ast$ denotes a convolitional operator, $\\cdot$ is an element-wise multiplication operator, $\\sigma(.)$ is a sigmoid function, $k$ is layer index, $f$ and $g$ denote filter and gate, respectively.\n",
    "\n",
    "\n",
    "### RESIDUAL AND SKIP CONNECTIONS\n",
    "\n",
    "Both residual and parameterized skip connections are used to enable trainig of much deeper models.\n",
    "\n",
    "<img src=\"img/WaveNet_A_Generative_Model_for_Raw_Audio_Figure4.png\">\n",
    "\n",
    "\n",
    "### CONDITIONAL WAVENETS\n",
    "\n",
    "Given an additional input $\\boldsymbol{h}$, WaveNet acn model conditional distribution $p(\\boldsymbol{x} \\lvert \\boldsymbol{h})$.\n",
    "\n",
    "$p(\\boldsymbol{x} \\lvert \\boldsymbol{h}) = \\prod_{t=1}^T p(x_t \\lvert x_1,...,x_{t-1},\\boldsymbol{h})$\n",
    "\n",
    "For example, by feeding a speeker identity speaker can be choosed in a multi-speeker settings.\n",
    "\n",
    "\n",
    "### CONTEXT STACKS\n",
    "\n",
    "In addition to incresing the number of dilation stages, using more layers, larger filters, greater dilation factors, complementary approach is to use separate, smaller context stack that processes a long part of the audio signal and locally conditions a larger WaveNet that processes only a smaller part of the audio signal.\n",
    "\n",
    "I don't understand this part. Does this mean SampleRNN like stacked structure?\n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "The generated audio can be found [here](https://deepmind.com/blog/wavenet-generative-model-raw-audio/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset used in this study\n",
    "\n",
    "- English multi-speaker corpus from CSTR voice cloning toolkit (VCTK)\n",
    "- single-speaker speech databases from which Google’s North American English and Mandarin Chinese TTS systems are built\n",
    "- TIMIT dataset for speech recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations\n",
    "\n",
    "- https://github.com/ibab/tensorflow-wavenet\n",
    "- https://github.com/tomlepaine/fast-wavenet\n",
    "- https://github.com/musyoku/wavenet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Readings\n",
    "\n",
    "a lot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
