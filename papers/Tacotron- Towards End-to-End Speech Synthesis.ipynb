{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tacotron: Towards End-to-End Speech Synthesis\n",
    "\n",
    "Link: https://arxiv.org/abs/1703.10135\n",
    "\n",
    "Authors: Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgiannakis, Rob Clark, Rif A. Saurous\n",
    "\n",
    "Institution: Google, Inc.\n",
    "\n",
    "Publication: arXiv\n",
    "\n",
    "Date: 6 Apr 2017\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Materials\n",
    "\n",
    "tacos & sushi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this paper about?\n",
    "\n",
    "\n",
    "Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the motivation of this research?\n",
    "\n",
    "Traditional TTS pipelines are complex. For example, it includes a text frontend extracting various linguistic features, a duration model, an acoustic feature prediction, and signal-processing based vocoder. They require domain expertise and labor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What makes this paper different from previous research?\n",
    "\n",
    "\n",
    "Tacotron uses seq2seq with attention but does not require phoneme-level alignment, so there is no need to use pre-trained aligner.\n",
    "\n",
    "Tacotron directly predict raw spectrogram so does not use vocoder.\n",
    "\n",
    "Tacotron is complete end-to-end model, which can learn directly from `<text, audio>` pair to predict spectrogram, so can be trained completely from scratch.\n",
    "\n",
    "Tacotron predicts frame level spectrograms, so it is significantly faster than sample level models but relatively low quality in terms of naturalness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this paper achieve it?\n",
    "\n",
    "Tacotron consists of:\n",
    "\n",
    "- character embedding layer\n",
    "- pre-net Encoder\n",
    "- CBHG Encoder\n",
    "- attention RNN\n",
    "- pre-net Decoder\n",
    "- Decoder RNN\n",
    "- CBHG post-processng net\n",
    "\n",
    "<img src=\"img/Tacotron-Towards_End-to-End_Speech_Synthesis_Figure1.png\" width=\"600\">\n",
    "\n",
    "### CBHG module\n",
    "\n",
    "CBHG module is a model inspired from machine translation (Lee et al., 2016).\n",
    "\n",
    "To model local and contextual information, the input sequence is first convolved with K sets of 1-D convolutional filters, where k-th set contains $C_k$ filters of width, which correspond to model K-gram.\n",
    "\n",
    "To increase local invariances, the convolution outputs are stacked together and max pooled along time.\n",
    "\n",
    "The processed sequence is passed to a few fixed-width 1-D convolutions. Its outputs are added with original sequence via residual connections.\n",
    "\n",
    "To extract high-level features, the convolution outputs are fed into a multi-layer highway network.\n",
    "\n",
    "To extract sequential features from both frward and backward context, the high-level feature outputs are fed into bidirectional GRU RNN.\n",
    "\n",
    "<img src=\"img/Tacotron-Towards_End-to-End_Speech_Synthesis_Figure2.png\" width=\"300\">\n",
    "\n",
    "\n",
    "\n",
    "### Encoder\n",
    "\n",
    "The goal of the encoder is to extract robust sequential representations of text. CBHG encoder reduces overfitting and makes fewer mispronunciations compared to a standard multi-layer RNN encoder.\n",
    "\n",
    "The input to the encoder is character sequence represented as one-hot vector, and they are embedded into continuous vector.\n",
    "\n",
    "\"pre net\" applies non-linear transformations to each embedding. A bottleneck layer with dropout is used in this study.\n",
    "\n",
    "\n",
    "\n",
    "### Decoder\n",
    "\n",
    "The purpose of the decoder is learning alignment between speech signal and text.\n",
    "\n",
    "A content-based tanh attention decoder (Vinyals et al., 2015) is used, where a stateful recurrent layer produces the attention query at each decoder timestep.\n",
    "\n",
    "The input to the decoder is a concatenation of the context vector and the attention RNN cell output.\n",
    "\n",
    "For decoder a stack of GRUs with vertical residual connections (Wu et al., 2016) is used.\n",
    "\n",
    "They did not choose a raw spectrogram as the decoder target. They chose 80-band mel-scale spectrogram as the seq2seq target. This redundancy makes it possible to use a different target for seq2seq decoding and waveform synthesis, that lead to a highly general model agnostic to waveform synthesis method.\n",
    "\n",
    "To predict the decoder target a simple fully-connected layer is used.\n",
    "\n",
    "\n",
    "\n",
    "### Post-processing net and waveform synthesis\n",
    "\n",
    "The task of the post-processing net is to convert the seq2seq target to target that can be synthesized into waveforms.\n",
    "\n",
    "A motivation of the post-processing net is that it can see the full decoded sequence both forward and backward to correct the prediction error, while seq2seq always runs from left-to-right.\n",
    "\n",
    "A CBHG module is used for post-processing net.\n",
    "\n",
    "In this work Griffin-Lim algorithm is used to synthesize waveform from predicted spectrogram. However the concept of a post-processing net is highly general, so can be used to predict alternative targets (e.g. vocoder parameter, WaveNet-like neural vocoder).\n",
    "\n",
    "### Experimental results\n",
    "\n",
    "Generated samples can be found at https://google.github.io/tacotron/ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset used in this study\n",
    "\n",
    "- internal North American English dataset (~ 24.6 hours)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations\n",
    "\n",
    "- https://github.com/Kyubyong/tacotron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Readings\n",
    "\n",
    "- Sequence to sequence learning with neural networks https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "- Highway Networks https://arxiv.org/abs/1505.00387\n",
    "- Fully Character-Level Neural Machine Translation without Explicit Segmentation https://arxiv.org/abs/1610.03017"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
