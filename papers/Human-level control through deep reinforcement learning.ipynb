{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [WIP] Human-level control through deep reinforcement learning\n",
    "\n",
    "Link: http://www.davidqiu.com:8888/research/nature14236.pdf\n",
    "\n",
    "Authors: Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg & Demis Hassabis\n",
    "\n",
    "Institution: Google DeepMind\n",
    "\n",
    "Publication: Nature Vol 518\n",
    "\n",
    "Date: 26 Feb. 2015\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Materials\n",
    "\n",
    "- https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this paper about?\n",
    "\n",
    "\n",
    "A new artificial agent, Deep Q-network, that can learn successful policies directly from video captures using end-to-end reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the motivation of this research?\n",
    "\n",
    "\n",
    "Applicability of traditional reinforcement learning agents has been limited to domain in which useful features can be handcrafted, or to domains with low-dimensional state spaces.\n",
    "\n",
    "Existing reinforce learning techniques based on DNN are unstable and inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What makes this paper different from previous research?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this paper achieve it?\n",
    "\n",
    "The goal of the agent is to select a action that maximizes cumulative future reward. \n",
    "\n",
    "This is done by approximating the optimal action-value function\n",
    "\n",
    "$Q(s, a) = \\max_{\\pi} \\mathbb{E}[r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ...  \\lvert s_t = s, a_t = a, \\pi]$\n",
    "\n",
    "which is the maximum sum of rewards $r_t$ discounted by $\\gamma$ at each time step $t$, achieved by a behaviour policy $\\pi = P(a\\lvert s)$, after making an observation $s$ and taking an action $a$.\n",
    "\n",
    "An approximate value function $Q(s,a;\\theta_i)$ is parameterized by $\\theta_i$, which is weights of the Q-network at iteration $i$.\n",
    "\n",
    "To stabilize reinforcement learning, \"experience replay\" is used, which is biologically inspired mechanism that randomize over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution. To perform experience replay the agent's experiences $e_t = (s_t, a_t, r_t, s_{t+1})$ are stored at each timestep $t$ in a data set $D_t = \\{e_1, ..., e_t\\}$. During learning, Q-learning update is applied on samples of experience $(s,a,r,s') \\sim U(D)$, drawn uniformly at random from the pool of stored samples.\n",
    "\n",
    "The loss function of the Q-learning update at iteration $i$ is\n",
    "\n",
    "$L_i(\\theta_i) = \\mathbb{E}_{(s,a,r,s') \\sim U(D)}[(r + \\gamma \\max_{a'}Q(s', a'; \\theta_{i}^-) - Q(s, a; \\theta_i)^2]$\n",
    "\n",
    "in which $\\gamma$ is discount factor, $\\theta_i$ are the parameter of the network at $i$, $\\theta_i^-$ are the network parameters used to compute the target at $i$. The target network parameter $\\theta_i^-$ are only updated with the Q-network parameters $\\theta_i$ every $C$ steps.\n",
    "\n",
    "The model has a output unit for each possible action. The state representation is in an input. The output correspond to the predicted Q-value of indivisual actions for the input state.\n",
    "\n",
    "The input consists of an 84 Ã— 84 preprocessed image.\n",
    "\n",
    "The model has three hidden convolutional layers, one fully-connected layer, and fully-connected linear output layer which size is the number of valid actions.\n",
    "\n",
    "<img src=\"img/Human-level_control_through_deep_reinforcement_learning_Figure1.png\" width=\"600\">\n",
    "\n",
    "The experiment depends on minimal  prior knowledge:\n",
    "\n",
    "- visual image input\n",
    "- the game specific score\n",
    "- number of actions\n",
    "- the life count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## target of experiments\n",
    "\n",
    "49 games of Atari 2600\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations\n",
    "\n",
    "- https://sites.google.com/a/deepmind.com/dqn/ (for non-commercial use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Readings\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
