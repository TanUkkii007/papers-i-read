{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Residual Learning for Image Recognition\n",
    "\n",
    "Link: https://arxiv.org/abs/1512.03385\n",
    "\n",
    "Authors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "\n",
    "Institution: Microsoft Research\n",
    "\n",
    "Publication: arXiv\n",
    "\n",
    "Date: 2015\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Materials\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this paper about?\n",
    "\n",
    "\n",
    "ResNet, \"very\" deep neural network architecture using residual learning, which is substantially deeper than those used previously for image recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the motivation of this research?\n",
    "\n",
    "Network depth is of crucial importance.\n",
    "\n",
    "However, deeper neural networks are more difficult to train.\n",
    "\n",
    "With the network depth increasing, accuracy gets aturated, and then degrades rapidly. This is called degradation problem.\n",
    "\n",
    "Let us consider shallower architecture and its deeper counterpart that add identity mapping layer onto it. The deeper model should produce no higher training error than its shallower counterpart. But experiments show that the deeper counterpart is unable to find solutions that are comparably better than shallower architecture.\n",
    "\n",
    "\n",
    "Figure1. Training error (left) and test error (right) on CIFAR-10.\n",
    "<img src=\"img/Deep_Residual_Learning_for_Image_Recognition_Figure1.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What makes this paper different from previous research?\n",
    "\n",
    "- using residual learning\n",
    "- very deep, up to 152 layers that is 8 times deeper than VGG\n",
    "- lower complexity even though the depth is significantly increased\n",
    "- low errors. ILSVRC 2015 winner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this paper achieve it?\n",
    "\n",
    "\n",
    "### Residual representations\n",
    "\n",
    "Denoting underlying mapping as $\\mathcal{H}(x)$, let the stacked nonlinear layers another mapping, residual function.\n",
    "\n",
    "$\\mathcal{F}(x) := \\mathcal{H}(x) - x$\n",
    "\n",
    "If one hypothesize that they can asymptotically approximate complicate funcions, it is equivalent to hypothesize that they can asymptotically approximate the residual function, $\\mathcal{H}(x) - x$ .\n",
    "\n",
    "The original function becomes \n",
    "\n",
    "$\\mathcal{F}(x) + x$\n",
    "\n",
    "Both forms should be able to asymtotically approximate the desired functions.\n",
    "\n",
    "The formulation $\\mathcal{F}(x) + x$ can be realized with identity mapping \"shortcut connections\" in feedforward neural network.\n",
    "\n",
    "<img src=\"img/Deep_Residual_Learning_for_Image_Recognition_Figure2.png\" width=\"300\">\n",
    "\n",
    "Identity shortcut connections add neither extra parameter nor computa- tional complexity.\n",
    "\n",
    "The degradation problem suggests that the solvers might have difficulities in approximating identity mapping by multiple nonlinear layers. With residual learning, if identity mappings are optimal, the solvers may simply drive the weights of multiple nonlinear layers toward zero to approach identity mappings.\n",
    "\n",
    "#### Related representation\n",
    "\n",
    "Vector of Locally Aggregated Descriptors (VLAD) representation is a way of producing compact representation of local visual descriptors while still retaining high level of accuracy (Je ́gou et al., 2010). As for Bag of Word, a visual vocabulary, called codebook, {$\\boldsymbol{\\mu}_1, ...,\\boldsymbol{\\mu}_K$} is first learned using a cluster algorithm such as  k-means. Each local descriptor $\\boldsymbol{x}_t$ is then associated with nearest visual word, or codeword $\\mathit{NN}(\\boldsymbol{x}_t)$ in the codebook. For each codeword the differences between the sub-vectors $x_t$ assigned to $\\boldsymbol{\\mu}_i$ are accumulated:\n",
    "\n",
    "$\\boldsymbol{v}_i = \\sum_{x_t:\\mathit{NN}(x_t)=i}\\boldsymbol{x}_t - \\boldsymbol{\\mu}_i$\n",
    "\n",
    "The VLAD is concatenation of the accumulated sub-vectors, $\\boldsymbol{V} = (\\boldsymbol{v}_1, ..., \\boldsymbol{v}_K)$\n",
    "\n",
    "Fisher Vector is probablistic variant of VLAD.\n",
    "\n",
    "For vector quantization, product quantization (Je ́gou et al., 2011) is effective encoding using residual vector. http://mglab.blogspot.jp/2011/11/product-quantization.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "<img src=\"img/Deep_Residual_Learning_for_Image_Recognition_Figure3r.png\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental results\n",
    "\n",
    "#### ImageNet Classification\n",
    "\n",
    "Figure 4. Training on ImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops.\n",
    "<img src=\"img/Deep_Residual_Learning_for_Image_Recognition_Figure4.png\" height=\"200\">\n",
    "\n",
    "In addition, 50/101/152-layer ResNets are experimented and show more accurate than 34-layer one.\n",
    "\n",
    "Remarkably, although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 (15.3/19.6 billion FLOPs).\n",
    "\n",
    "The 152-layer variant has 3.57% top-5 error on the test set and won the 1st place in ILSVRC 2015.\n",
    "\n",
    "\n",
    "#### CIFAR-10 analysis\n",
    "\n",
    "1202-layer network shows no optimization difficulty and achieve training error < 0.1% and test error 7.93%. This result is worse than that ou 110-layer network. This is suspected to be overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset used in this study\n",
    "\n",
    "- ImageNet\n",
    "- CIFAR-10\n",
    "- PASCAL VOC 2007 and 2012\n",
    "- MS COCO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations\n",
    "\n",
    "- [tensorflow/models/resnet](https://github.com/tensorflow/models/tree/master/resnet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Readings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
