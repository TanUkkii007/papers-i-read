{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation\n",
    "\n",
    "Link: https://arxiv.org/abs/1406.1078\n",
    "\n",
    "Authors: Kyunghyun Cho, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio\n",
    "\n",
    "Institution: Universite ́ de Montre ́al, Jacobs University, Universite ́ du Maine \n",
    "\n",
    "Publication: arXiv\n",
    "\n",
    "Date: 3 Sep. 2014\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Materials\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this paper about?\n",
    "\n",
    "A new neural network model called RNN Encoder-Decoder is applyed to a statistical machine translation.\n",
    "\n",
    "In RNN Encoder-Decoder, one RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the motivation of this research?\n",
    "\n",
    "The previous researches are limited in a sense that:\n",
    "\n",
    "- the input sequences should be fixed length\n",
    "- the order of input sequences are not taken into account because inputs are represented as bag-of-words\n",
    "- the input is n-gram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What makes this paper different from previous research?\n",
    "\n",
    "\n",
    "The RNN Encode-Decoder can take a variable-length input that is a raw symbol sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this paper achieve it?\n",
    "\n",
    "### RNN Encoder–Decoder\n",
    "\n",
    "The encoder reads each symbol of an input sequence $x$ sequentially and the hidden state of RNN changes as follows,\n",
    "\n",
    "$\\boldsymbol{h}_{<t>} = f(\\boldsymbol{h}_{<t-1>}, x_t)$\n",
    "\n",
    "where $f$ is a non-linear activation function.\n",
    "\n",
    "After reading the end of the sequence, the hidden state is a summary $c$ of the whole input sequence.\n",
    "\n",
    "\n",
    "The decoder is another RNN which predicts the next symbol $y_t$ given the hidden state $\\boldsymbol{h}_{<t>}$.\n",
    "\n",
    "In the decoder RNN both $y_t$ and $\\boldsymbol{h}_{<t>}$ are conditioned on $y_{t-1}$ and on summary $c$. Hence, the hidden state of the decoder RNN at time t is,\n",
    "\n",
    "$\\boldsymbol{h}_{<t>} = f(\\boldsymbol{h}_{<t-1>}, y_{t-1}, \\boldsymbol{c})$\n",
    "\n",
    "Similarly, the conditional distribution of the next symbol is \n",
    "\n",
    "$P(y_t \\lvert y_{t-1}, y_{t-2}, ..., y_1, \\boldsymbol{c}) = g(\\boldsymbol{h}_{<t>}, y_{t-1}, \\boldsymbol{c})$\n",
    "\n",
    "where $g$ is probability function like softmax.\n",
    "\n",
    "<img src=\"img/Learning Phrase Representations using RNN Encoder–Decoder_for_Statistical_Machine_Translation_figure1.png\" width=\"300\">\n",
    "\n",
    "\n",
    "The two RNNs are jointly trained to maximize the conditional log-likelihood\n",
    "\n",
    "$\\max_\\theta \\frac{1}{N}\\sum_{n=1}^N \\log p_\\theta(\\boldsymbol{y}_n \\lvert \\boldsymbol{x}_n)$\n",
    "\n",
    "where $\\theta$ is the set of the model parameters.\n",
    "\n",
    "\n",
    "\n",
    "### Statistical Machine Translation\n",
    "\n",
    "A common statistical machine translation system aims to find a translation $\\boldsymbol{f}$ given source sentence $\\boldsymbol{e}$ which maximizes\n",
    "\n",
    "$p(\\boldsymbol{f} \\lvert \\boldsymbol{e}) \\propto p(\\boldsymbol{e} \\lvert \\boldsymbol{f}) p(\\boldsymbol{f})$ (from Bays theorem)\n",
    "\n",
    "where $p(\\boldsymbol{e} \\lvert \\boldsymbol{f})$ is called translation model and $p(\\boldsymbol{f})$ language model.\n",
    "\n",
    "In practice, however, most SMT systems model $\\log p(\\boldsymbol{f} \\lvert \\boldsymbol{e})$ as log-linear model with additional features and corresponding weights:\n",
    "\n",
    "$\\log p(\\boldsymbol{f} \\lvert \\boldsymbol{e}) = \\sum_{n=1}^N \\omega_n f_n(\\boldsymbol{f}, \\boldsymbol{e}) + \\log Z(\\boldsymbol{e})$\n",
    "\n",
    "where $f_n$ and $w_n$ are the $n$-th feature and weight, respectively. $Z(\\boldsymbol{e})$ is normalized constant that does not depend on the weights. The weights are often optimized to maximize the BLEU score on a training set.\n",
    "\n",
    "The RNN Encoder-Decoder trained on a table of phrase pairs can be used to score a given pair of input and output sequences. The score is simply a probability $p_\\theta(\\boldsymbol{y} \\lvert \\boldsymbol{x})$.\n",
    "\n",
    "The score is used  as additional features in the log-linear model when tuning SMT decoder. Once the RNN Encoder-Decoder is trained, a new score for each phrase pair is added to the existing table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset used in this study\n",
    "\n",
    "\n",
    "- WMT’14 translation task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Readings\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
