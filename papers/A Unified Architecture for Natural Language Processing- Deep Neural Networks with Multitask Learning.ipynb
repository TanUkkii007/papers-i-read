{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Unified Architecture for Natural Language Processing- Deep Neural Networks with Multitask Learning\n",
    "\n",
    "Link: https://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf\n",
    "\n",
    "Authors: Ronan Collobert, Jason Weston\n",
    "\n",
    "Institution: NEC Labs America\n",
    "\n",
    "Publication: Proceedings of the 25th International Confer- ence on Machine Learning\n",
    "\n",
    "Date: 2008\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Materials\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Papers citing this paper\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this paper about?\n",
    "\n",
    "A single convolutional neural network architecture that outputs part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and likelihood that the sentence makes sense grammatically and semantically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the motivation of this research?\n",
    "\n",
    "*Separately* analyzing NLP tasks pose following failings:\n",
    "- shallow in the sense that the classifier is often linear\n",
    "- requires many hand-engeneered features specific for the task for good performance\n",
    "- propagating errors by cascading features lernt separately from other tasks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What makes this paper different from previous research?\n",
    "\n",
    "They define a *unified* architecture for NLP that learns relevant features between tasks given *very limited prior knowledge*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this paper achieve it?\n",
    "\n",
    "### NLP tasks\n",
    "\n",
    "#### Part-Of-Speech Tagging (POS)\n",
    "\n",
    "Labeling each word with a qunique tag that indicates its syntactic role (e.g. noun, adverb).\n",
    "\n",
    "#### Chunking\n",
    "\n",
    "Labeling segments of a sentence with syntactic constituents (e.g. noun phrase NP, verb phrase VP).\n",
    "\n",
    "#### Named Entity Recognition (NER)\n",
    "\n",
    "Labeling atomic elementsin the sentence into categories (e.g. \"PERSON\", \"COMPANY\", \"LOCATION\")\n",
    "\n",
    "#### Semantic Role Labeling (SRL)\n",
    "\n",
    "Giving a semantic role to a syntactic constituent of a sentence. In the PropBank formalism, one assigns roles ARG0-5 to words that are arguments of a predicate in the sentense. (e.g. $[\\mathrm{John}]_{\\mathrm{ARG0}} [\\mathrm{ate}]_\\mathrm{REL} [\\mathrm{the\\ apple}]_{\\mathrm{ARG1}}$)\n",
    "\n",
    "#### Language Models\n",
    "\n",
    "Estimating the probability of the next word being $w$ in a sentence.\n",
    "\n",
    "#### Semantically Related Words (\"Synonyms\")\n",
    "\n",
    "Predicting whether two words are semantically related, which is measured WordNet database.\n",
    "\n",
    "\n",
    "### General Deep Architecture for NLP\n",
    "\n",
    "A deep neural network is used and traind in end-to-end fashion.\n",
    "\n",
    "The first layer extract features for each word. The second layer extracts features from the sentence treating it as a sequence with local and global structure. The following layers are classical NN layers.\n",
    "\n",
    "<img src=\"img/A_Unified_Architecture_for_Natural_Language_Processing-Deep_Neural_Networks_with_Multitask_Learning_Figure1.png\" width=\"300\">\n",
    "\n",
    "#### Transforming Indices into Vectors\n",
    "\n",
    "The first layer directly deal with raw words. The words are mapped into a vectors.\n",
    "\n",
    "Each word $i \\in \\mathcal{D}$ is embedded into d-dimensional space using a lookup table $\\mathrm{LT}_W(.)$:\n",
    "\n",
    "$\\mathrm{LT}_W(i) = W_i$\n",
    "\n",
    "where $\\mathcal{D}$ is a finit dictionary of words, $W \\in \\mathbb{R}^{d\\times |\\mathcal{D}|}$ is a matrix of parameters to be learnt, $W_i \\in \\mathbb{R}^d$ is the i-th column of $W$ and $d$ is the word vector size (wsz) to be chosen by the user.\n",
    "\n",
    "An input sentence $\\{s_1,..., s_n\\}$ is this transformed into a series of vectors $\\{W_{s_1}, ..., W_{s_n}\\}$.\n",
    "\n",
    "The parameter $W$ is trained during learning process.\n",
    "\n",
    "#### Variations on Word Representations\n",
    "\n",
    "All words are converted to lower case and capitalization are represented as separate feature flag.\n",
    "\n",
    "When a word is decomposed into K deatures, it is represented as a tuple $\\boldsymbol{i} = \\{i^1,...,i^K\\} \\in \\mathcal{D}^1\\times ... \\times \\mathcal{D}^K$, where $\\mathcal{D}^K$ is the dictionary for the k-th element.\n",
    "\n",
    "Each element is associated to a lookup-table $\\mathrm{LT}_{W^k}(.)$ with parameters $W^k \\in \\mathbb{R}^{d\\times |\\mathcal{D^k}|}$.\n",
    "\n",
    "#### Classifying with Respect to a Predicate\n",
    "\n",
    "In SRL the class label of each word in a sentence depends on a given predicate.\n",
    "\n",
    "A feature that encodes its relative distance to the predicate is added. For the i-th word in a sentence, if the predicate is at position $\\mathrm{pos_p}$ additional lookup table $LT^\\mathrm{dist_p}(i - \\mathrm{pos_p})$\n",
    "\n",
    "\n",
    "#### Variable Sentence Length\n",
    "\n",
    "To deal with variable-length sequence, Time-Delay Neural Networks (TDNNs, Waibel et al., 1989) is used.\n",
    "\n",
    "A TDNN \"reads\" the sequence in an online fashion, at time $t$ one sees $x_t$.\n",
    "\n",
    "A classical TDNN layer performs a convolution on a given sequence $\\boldsymbol{o}$.\n",
    "\n",
    "$\\boldsymbol{o}(t) = \\sum_{j=1-t}^{n-t} \\boldsymbol{L}_j x_{t+j}$\n",
    "\n",
    "where $L_j \\in \\mathbb{R}^{n_{hu}\\times d} (-n \\le j \\le n)$ are the training parameters of the layer with $n_{hu}$ hidden units.\n",
    "\n",
    "The convolution is constrained by defining a *kernel witdth* (ksz), which enforces\n",
    "\n",
    "$\\forall |j| \\gt (ksz - 1)/2, \\boldsymbol{L}_j = 0$\n",
    "\n",
    "Unlike window approach, TDNN considers at the same time all windows of ksz words in the sentence, whereas window approach only considers words in a window of size ksz around the word.\n",
    "\n",
    "As the layer's output is fixed dimension, subsequent layers can be classical NN layers.\n",
    "\n",
    "#### Deep Architecture\n",
    "\n",
    "A TDNN layer performs a linear operation.\n",
    "\n",
    "A nonlinearity is added by $\\tanh$ activation.\n",
    "\n",
    "$\\boldsymbol{o}^l = \\tanh(\\boldsymbol{L^l} \\cdot \\boldsymbol{o^{t-1}})$\n",
    "\n",
    "The size of last layer's output is the number of classes of the NLP task. The last layer is followed by softmax layer and trained with the cross-entropy criterion.\n",
    "\n",
    "\n",
    "### Multitasking with Deep NN\n",
    "\n",
    "#### Deep Joint Training\n",
    "\n",
    "If one considers related tasks, features useful for one task might be useful for other ones.\n",
    "\n",
    "In NLP, POS prediction are often used as features for SRL and NER.\n",
    "\n",
    "It is expected that when training NNs on related tasks sharing deep layers in these NNs would improve generalization performance.\n",
    "\n",
    "#### Previous Work in MTL for NLP\n",
    "\n",
    "The two types of previsous multi-task learning research exists.\n",
    "\n",
    "- cascading features\n",
    "- shallow joint training\n",
    "\n",
    "\n",
    "### Leveraging Unlabeled Data\n",
    "\n",
    "The proposed architecture can be jointly trained  supervised tasks on labeled data and unsupervised tasks on unlabeled data.\n",
    "\n",
    "#### Language Model\n",
    "\n",
    "They trained a language model that discriminates a two-class classification task: if the word in the middle of the input window is related to its context or not.\n",
    "\n",
    "Their experiments showed that the embedding learnt by the lookup-table layer clusters semantically similar words. \n",
    "\n",
    "The resulting word lookup-table from the language model was used as an initializer of lookup-table in MTL.\n",
    "\n",
    "<img src=\"img/A_Unified_Architecture_for_Natural_Language_Processing-Deep_Neural_Networks_with_Multitask_Learning_Figure2.png\" width=\"400\">\n",
    "\n",
    "#### Semantically Related Words Task\n",
    "\n",
    "The embedding obtained with a language model on unlabeled data and an embedding obtained with labeled data are compared.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset used in this study\n",
    "\n",
    "- PropBank dataset version 1\n",
    "- Penn TreeBank\n",
    "- Wikipedia\n",
    "- WordNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Readings\n",
    "\n",
    "- http://sebastianruder.com/multi-task/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
